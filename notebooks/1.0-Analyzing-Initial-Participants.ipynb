{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Analyzing Initial Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiler    : GCC 12.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-117-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: apra-desktop-ubuntu\n",
      "\n",
      "Git hash: 2af72ed6d09ec71dd7758a7d6f41dec7d980da17\n",
      "\n",
      "Git branch: main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -m -g -b -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.12.4\n",
      "IPython version      : 8.26.0\n",
      "\n",
      "numpy  : 2.0.1\n",
      "json   : 2.0.9\n",
      "pandas : 2.2.2\n",
      "seaborn: 0.13.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "# Quickly reference all git tracked folders in the\n",
    "import index\n",
    "\n",
    "# Display  versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Participant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"hbb_dataset_240409_121323\"\n",
    "study_id = \"669e784b617d540aa357abf4\"\n",
    "path_study_id = index.dir_data_participant_responses / study_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading participant 66b030a3b56fc1387defa633...\n",
      "Found 285 responses and 0 miss(es)\n",
      "Loading participant 669ead0b8baa798838ac2787...\n",
      "Found 284 responses and 1 miss(es)\n",
      "Loading participant 653bbdaa0cf432b1c544b303...\n",
      "Found 285 responses and 0 miss(es)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126321/2954991046.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_response_filtered[column] = df_response_filtered[column].astype(int)\n",
      "/tmp/ipykernel_126321/2954991046.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_response_filtered[column] = df_response_filtered[column].astype(int)\n",
      "/tmp/ipykernel_126321/2954991046.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_response_filtered[column] = df_response_filtered[column].astype(int)\n"
     ]
    }
   ],
   "source": [
    "def load_all_participant_responses(\n",
    "    path_study_id, \n",
    "    dataset_name,\n",
    "    validate=True,\n",
    "    valid_colors={1: \"red\", 2: \"green\", 3: \"blue\"},\n",
    "    columns_to_keep=(\n",
    "        \"rt\",\n",
    "        \"response\",\n",
    "        \"trial_index\",\n",
    "        \"internal_node_id\",\n",
    "        \"correct_response\",\n",
    "    ),\n",
    "):\n",
    "    participant_ids = [path.stem for path in (path_study_id).iterdir()]\n",
    "    participant_jsons = {\n",
    "        participant_id : list((path_study_id / participant_id).iterdir())[0]\n",
    "        for participant_id in participant_ids\n",
    "    }\n",
    "\n",
    "    participant_responses = {}\n",
    "\n",
    "    for participant_id, participant_json in participant_jsons.items():\n",
    "        print(f\"Loading participant {participant_id}...\")\n",
    "        with open(participant_json) as f:\n",
    "            json_participant_data = json.load(f)\n",
    "\n",
    "        df_participant_data = pd.DataFrame(json_participant_data[\"trials\"])\n",
    "    \n",
    "        # Filter data based on which entries has a correct response value\n",
    "        df_response = df_participant_data.loc[~df_participant_data.correct_response.isna()]\n",
    "        \n",
    "        # Get the paths to all the videos\n",
    "        path_videos = (\n",
    "            df_participant_data[np.logical_or(\n",
    "                    df_participant_data.trial_type == \"video-keyboard-response\",\n",
    "                    df_participant_data.trial_type == \"video-button-response\"\n",
    "                )\n",
    "            ]\n",
    "            .stimulus.apply(lambda x: str(index.dir_public / x[0]))\n",
    "            .to_list()\n",
    "        )\n",
    "        \n",
    "        # Only keep the desired columns\n",
    "        df_response = df_response[list(columns_to_keep)]\n",
    "        \n",
    "        # Add in the path to videos\n",
    "        df_response.loc[:, \"Path Video\"] = path_videos\n",
    "\n",
    "        # Subselect off those the ones that have a response value\n",
    "        isna = df_participant_data.response.isna()\n",
    "        \n",
    "        df_response_filtered = df_response.loc[~isna]\n",
    "        df_response_dict = {\"missed\" : df_response.loc[isna]}\n",
    "        path_videos_filtered = df_response_filtered[\"Path Video\"].tolist()\n",
    "        print(f\"Found {len(df_response_filtered)} responses and {len(df_response_dict[\"missed\"])} miss(es)\")\n",
    "\n",
    "        # Do some type conversions\n",
    "        columns_astype_int = [\"rt\", \"response\", \"correct_response\"]\n",
    "        columns_astype_int = [\n",
    "            col for col in columns_astype_int if col in columns_to_keep\n",
    "        ]\n",
    "        for column in columns_astype_int:\n",
    "            df_response_filtered[column] = df_response_filtered[column].astype(int)\n",
    "\n",
    "        if validate:\n",
    "            # All collected responses are in the valid responses\n",
    "            assert all(\n",
    "                val in valid_colors.keys() for val in df_response_filtered.response.unique()\n",
    "            )\n",
    "        \n",
    "            # Compare the path colors to the correct_responses\n",
    "            colors_from_video_paths = [\n",
    "                Path(path).stem.split(\"_\")[-1] for path in path_videos_filtered\n",
    "            ]\n",
    "            colors_from_correct_response = [\n",
    "                valid_colors[val] for val in df_response_filtered.correct_response\n",
    "            ]\n",
    "            assert colors_from_video_paths == colors_from_correct_response\n",
    "    \n",
    "        index_videos_walkthrough = [\n",
    "            i for i, path in enumerate(path_videos_filtered) if \"walkthrough\" in path.split(\"/\")\n",
    "        ]\n",
    "        df_response_dict[\"walkthrough\"] = df_response_filtered.iloc[index_videos_walkthrough]\n",
    "        \n",
    "        index_videos_examples = [\n",
    "            i for i, path in enumerate(path_videos_filtered) if \"examples\" in path.split(\"/\")\n",
    "        ]\n",
    "        df_response_dict[\"examples\"] = df_response_filtered.iloc[index_videos_examples]\n",
    "        \n",
    "        index_videos_experiment = [\n",
    "            i for i in range(len(path_videos_filtered)) \n",
    "            if i not in index_videos_walkthrough + index_videos_examples\n",
    "        ]\n",
    "        df_response_dict[\"experiment\"] = df_response_filtered.iloc[index_videos_experiment]\n",
    "        \n",
    "        participant_responses[participant_id] = df_response_dict\n",
    "\n",
    "    return participant_responses\n",
    "\n",
    "all_participant_responses = load_all_participant_responses(\n",
    "    path_study_id,\n",
    "    dataset_name,\n",
    "    validate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortening the Participant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'66b030': '66b030a3b56fc1387defa633',\n",
       " '669ead': '669ead0b8baa798838ac2787',\n",
       " '653bbd': '653bbdaa0cf432b1c544b303'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_length = 6\n",
    "participant_ids = all_participant_responses.keys()\n",
    "participant_ids_short_dict = {\n",
    "    participant_id[:shortened_length] : participant_id for participant_id in participant_ids\n",
    "}\n",
    "participant_ids_short_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participant_responses = {\n",
    "    key[:shortened_length] : val \n",
    "    for key, val in all_participant_responses.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Walkthrough</th>\n",
       "      <th>Accuracy Examples</th>\n",
       "      <th>Accuracy Experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66b030</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669ead</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.587591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653bbd</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.552727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy Walkthrough  Accuracy Examples  Accuracy Experiment\n",
       "66b030                   1.0                0.8             0.636364\n",
       "669ead                   1.0                0.4             0.587591\n",
       "653bbd                   1.0                0.4             0.552727"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_accuracies = {}\n",
    "for participant_id, responses_dict in all_participant_responses.items():\n",
    "    participant_accuracies[participant_id] = {\n",
    "        f\"Accuracy {key.title()}\" : (df.correct_response == df.response).mean()\n",
    "        for key, df in responses_dict.items()\n",
    "        if key != \"missed\"\n",
    "    }\n",
    "\n",
    "pd.DataFrame.from_dict(participant_accuracies, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
